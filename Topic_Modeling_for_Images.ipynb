{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUQgdiSX6g0j"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5xytMyOAHVu"
      },
      "source": [
        "# Common data handling libraries\n",
        "import os\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import pickle\n",
        "\n",
        "# Gensim for LDA\n",
        "import gensim\n",
        "\n",
        "# NLTK for test processing\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# spacy for Lemmatization\n",
        "import spacy\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Deep learning modeling\n",
        "import keras\n",
        "\n",
        "# Model evaluation metrics\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyIXpxOnOi1P"
      },
      "source": [
        "import pandas as pd\n",
        "img_cap_df2 = pd.read_csv('tweetsTRAIN.txt', delimiter = \"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "romXQA62Olxl"
      },
      "source": [
        "img_cap_df2['caption'] = img_cap_df2['tweetText']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4Pgk3EBSHDi"
      },
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "from pathlib import Path\n",
        "path = \"TweetsTRAINImages/\"\n",
        "\n",
        "lab = []\n",
        "imgg = []\n",
        "ca = []\n",
        "cap = img_cap_df2['caption'].to_list()\n",
        "im = img_cap_df2['imageId(s)'].to_list()\n",
        "labels = img_cap_df2['label'].to_list()\n",
        "for i in range(len(img_cap_df2['tweetId'].to_list())):\n",
        "  try:\n",
        "    filepath = 'TweetsTRAINImages/'+ im[i]+'.jpg'\n",
        "    img = Image.open(filepath).resize((128,128), Image.BICUBIC).convert('RGB')\n",
        "    ca.append(cap[i])\n",
        "    lab.append(labels[i])\n",
        "    imgg.append(filepath)\n",
        "  except:\n",
        "    print(\"Image at filepath {0} does not exist\".format(filepath))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JrkNjW5TKlw"
      },
      "source": [
        "img_cap_df = pd.DataFrame({'image_id': imgg,\n",
        "                   'caption': ca,\n",
        "                   'label': lab\n",
        "\n",
        "\n",
        "                   })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzyrSHopAHVx"
      },
      "source": [
        "def clean_text(data):\n",
        "\n",
        "    # convert to lower case\n",
        "    data = [word.lower() for word in data.split()]\n",
        "\n",
        "    # prepare translation table for removing punctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    # remove punctuation from each word\n",
        "    data = [word.translate(table) for word in data]\n",
        "\n",
        "    # remove tokens with numbers in them\n",
        "    data = [word for word in data if word.isalpha()]\n",
        "\n",
        "    # remove stopwords\n",
        "    data = [word for word in data if word not in nltk.corpus.stopwords.words('english')]\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "data_caption = list(img_cap_df['caption'].apply(lambda x : clean_text(x)))\n",
        "data_caption[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqPDVttwUS6g"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtwbCnnFUVrN"
      },
      "source": [
        "!python -m spacy validate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgIHQy0kUY1R"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZOYXuK3AHVy"
      },
      "source": [
        "# lemmatize the words\n",
        "#nlp = spacy.load(r\"c:\\users\\jsaikumar\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.2.0\", disable=['parser', 'ner'])\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
        "data_caption_lemmatized = [[word.lemma_ for word in nlp(str(' '.join(doc))) if word.pos_ in allowed_postags]\n",
        "                           for doc in data_caption]\n",
        "data_caption_lemmatized[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNmfcDxXAHVy"
      },
      "source": [
        "img_cap_df['caption_lemmatized'] = data_caption_lemmatized\n",
        "img_cap_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXCj4O9vAHVy"
      },
      "source": [
        "# Train, valid, test split of dataset\n",
        "train_df = img_cap_df[:6000]\n",
        "valid_df = img_cap_df[6000:7000]\n",
        "test_df = img_cap_df[7000:8000]\n",
        "print(train_df.shape)\n",
        "print(valid_df.shape)\n",
        "print(test_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNGXS8fcAHVz"
      },
      "source": [
        "# Create Dictionary\n",
        "id2word = gensim.corpora.Dictionary(train_df['caption_lemmatized'])\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in train_df['caption_lemmatized']]\n",
        "\n",
        "# View\n",
        "print(corpus[:1])\n",
        "\n",
        "# Human readable format of corpus (term-frequency)\n",
        "print([[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybTuxjIlAHVz"
      },
      "source": [
        "# Find the optimal number of topics\n",
        "START = 10\n",
        "LIMIT = 100\n",
        "STEP = 5\n",
        "topic_range = range(START, LIMIT, STEP)\n",
        "\n",
        "coherence_values = []\n",
        "model_list = []\n",
        "for num_topics in topic_range:\n",
        "    model = gensim.models.LdaMulticore(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
        "    model_list.append(model)\n",
        "    coherencemodel = gensim.models.coherencemodel.CoherenceModel(model=model, texts=train_df['caption_lemmatized'],\n",
        "                                                                 dictionary=id2word, coherence='c_v')\n",
        "    coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "max_coherence_val = 0\n",
        "optimal_model = None\n",
        "\n",
        "# Print the coherence scores\n",
        "for i, (m, cv) in enumerate(zip(topic_range, coherence_values)):\n",
        "    if max_coherence_val < round(cv, 4):\n",
        "        optimal_model = model_list[i]\n",
        "        optimal_num_topics = m\n",
        "        max_coherence_val = round(cv, 4)\n",
        "\n",
        "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
        "\n",
        "# plot coherence results\n",
        "plt.plot(coherence_values)\n",
        "plt.xlabel(\"Number of Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obnfrlXEAHVz"
      },
      "source": [
        "# Print the Keyword in the 10 topics\n",
        "for topic in optimal_model.print_topics():\n",
        "    print(topic)\n",
        "\n",
        "doc_lda = optimal_model[corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq4dhFf0HiNy"
      },
      "source": [
        " optimal_num_topics =  95"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRcegeFxAHV0"
      },
      "source": [
        "print('Optimal Number of Topics :', optimal_num_topics)\n",
        "\n",
        "# Compute Perplexity\n",
        "print('\\nPerplexity: ', optimal_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=optimal_model, texts=data_caption_lemmatized,\n",
        "                                                                  dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AiDvzCaAHV0"
      },
      "source": [
        "def predictTopics(corpus):\n",
        "\n",
        "    caption_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row in enumerate(optimal_model[corpus]):\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = optimal_model.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                caption_topics_df = caption_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]),\n",
        "                                                             ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    caption_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "    return caption_topics_df\n",
        "\n",
        "df = predictTopics([id2word.doc2bow(text) for text in train_df['caption_lemmatized']])\n",
        "train_df = pd.concat([train_df.reset_index(drop=True), df], axis=1)\n",
        "\n",
        "df = predictTopics([id2word.doc2bow(text) for text in valid_df['caption_lemmatized']])\n",
        "valid_df = pd.concat([valid_df.reset_index(drop=True), df], axis=1)\n",
        "\n",
        "df = predictTopics([id2word.doc2bow(text) for text in test_df['caption_lemmatized']])\n",
        "test_df = pd.concat([test_df.reset_index(drop=True), df], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaKwgZbaAHV0"
      },
      "source": [
        "# save the data object files\n",
        "pickle.dump(train_df, open('./data/train_df.pkl','wb'))\n",
        "pickle.dump(valid_df, open('./data/valid_df.pkl','wb'))\n",
        "pickle.dump(test_df, open('./data/test_df.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfZjvXLwAHV0"
      },
      "source": [
        "# Load the data object files\n",
        "optimal_num_topics =\n",
        "train_df = pickle.load(open('./data/train_df.pkl', 'rb'))\n",
        "valid_df = pickle.load(open('./data/valid_df.pkl', 'rb'))\n",
        "test_df = pickle.load(open('./data/test_df.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g88fbRX9DqmC"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWH3JPFCkBfe"
      },
      "source": [
        "optimal_num_topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2PKF13DAHV1"
      },
      "source": [
        "# Create model\n",
        "vgg16_model = tf.keras.applications.VGG16(weights='imagenet', include_top=True, input_shape=(224,224,3))\n",
        "\n",
        "# pop the last softmax layer\n",
        "vgg16_model.layers.pop()\n",
        "\n",
        "# freezing the remaining layers\n",
        "for layer in vgg16_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "output_model = keras.layers.Dense(2056, activation='tanh')(vgg16_model.layers[-1].output)\n",
        "output_model = keras.layers.Dropout(0.5)(output_model)\n",
        "output_model = keras.layers.Dense(1024, activation='tanh')(output_model)\n",
        "output_model = keras.layers.Dropout(0.5)(output_model)\n",
        "output_model = keras.layers.Dense(optimal_num_topics, activation='softmax')(output_model)\n",
        "\n",
        "vgg16_model = tf.keras.Model(vgg16_model.input,output_model)\n",
        "\n",
        "vgg16_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "vgg16_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiZFjmoxAHV1"
      },
      "source": [
        "import pathlib\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, images_paths, labels, image_dimensions=(224, 224, 3), batch_size=64, shuffle=False):\n",
        "        self.labels       = labels              # array of labels\n",
        "        self.images_paths = images_paths        # array of image paths\n",
        "        self.image_dim = image_dimensions\n",
        "        self.batch_size   = batch_size          # batch size\n",
        "        self.shuffle      = shuffle             # shuffle bool\n",
        "\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.images_paths) / self.batch_size))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.images_paths))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # selects indices of data for next batch\n",
        "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "\n",
        "        # select data and load images\n",
        "        labels = np.array([self.labels[k] for k in indexes])\n",
        "\n",
        "        images = np.array([self.preprocessImageForVGG16(self.images_paths[k]) for k in indexes ])\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "\n",
        "    #customize function used for color convetion\n",
        "    def preprocessImageForVGG16(self, filename):\n",
        "        # load image\n",
        "        image = keras.preprocessing.image.load_img(filename, target_size=(self.image_dim[0], self.image_dim[1]))\n",
        "        # convert the image pixels to a numpy array\n",
        "        image = keras.preprocessing.image.img_to_array(image)\n",
        "        # prepare the image for the VGG model\n",
        "        image = keras.applications.vgg16.preprocess_input(image)\n",
        "\n",
        "        return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kmpIlvLE1Xp"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "DSYtCeJyAHV1"
      },
      "source": [
        "# Train the model\n",
        "# reduces learning rate if no improvement are seen\n",
        "learning_rate_reduction = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                            patience=2,\n",
        "                                            verbose=1,\n",
        "                                            factor=0.5,\n",
        "                                            min_lr=0.0000001)\n",
        "\n",
        "# stop training if no improvements are seen\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
        "                           mode=\"min\",\n",
        "                           patience=10)\n",
        "\n",
        "# saves model weights to file\n",
        "checkpoint = keras.callbacks.ModelCheckpoint('topic_predictor_model.hdf5',\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='min',\n",
        "                             save_weights_only=True)\n",
        "\n",
        "\n",
        "X_train = train_df['image_id'].values\n",
        "Y_train = train_df['Dominant_Topic'].values\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, num_classes=optimal_num_topics)\n",
        "\n",
        "X_valid = valid_df['image_id'].values\n",
        "Y_valid = valid_df['Dominant_Topic'].values\n",
        "Y_valid = tf.keras.utils.to_categorical(Y_valid, num_classes=optimal_num_topics)\n",
        "\n",
        "\n",
        "# prepare data generator\n",
        "train_data = DataGenerator(X_train, Y_train, batch_size=50, shuffle=True)\n",
        "valid_data = DataGenerator(X_valid, Y_valid, batch_size=50, shuffle=False)\n",
        "\n",
        "# train on data\n",
        "history = vgg16_model.fit_generator(generator=train_data,\n",
        "                                   validation_data=valid_data,\n",
        "                                   epochs=20,\n",
        "                                   steps_per_epoch=len(train_data),\n",
        "                                   validation_steps =len(valid_data),\n",
        "                                   callbacks=[learning_rate_reduction, early_stop, checkpoint],\n",
        "                                   verbose=2,\n",
        "                                   )\n",
        "\n",
        "# plot training history\n",
        "fig, ax = plt.subplots(2, 1, figsize=(6, 6))\n",
        "ax[0].plot(history.history['loss'], label=\"TrainLoss\")\n",
        "ax[0].plot(history.history['val_loss'], label=\"ValLoss\")\n",
        "ax[0].legend(loc='best', shadow=True)\n",
        "\n",
        "ax[1].plot(history.history['acc'], label=\"TrainAcc\")\n",
        "ax[1].plot(history.history['val_acc'], label=\"ValAcc\")\n",
        "ax[1].legend(loc='best', shadow=True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDdXxekcAHV1"
      },
      "source": [
        "vgg16_model.load_weights('topic_predictor_model.hdf5')\n",
        "\n",
        "X_test = test_df['image_id'].values\n",
        "Y_test = test_df['Dominant_Topic'].values\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, num_classes=optimal_num_topics)\n",
        "\n",
        "# prepare data generator\n",
        "test_data = DataGenerator(X_test, Y_test, batch_size=1, shuffle=False)\n",
        "\n",
        "# predict on data\n",
        "pred_caption_topics_prob = vgg16_model.predict_generator(test_data)\n",
        "pred_caption_topics = np.argmax(pred_caption_topics_prob, axis=1)\n",
        "\n",
        "results_df = pd.DataFrame({ 'image_id':X_test, 'pred_topics':pred_caption_topics })\n",
        "results_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiu0314VAHV2"
      },
      "source": [
        "# Evaluation score\n",
        "log_score = sklearn.metrics.log_loss(test_df['Dominant_Topic'].values, pred_caption_topics_prob)\n",
        "print('Log-loss score :', log_score)\n",
        "\n",
        "acc_score = sklearn.metrics.accuracy_score(test_df['Dominant_Topic'].values, pred_caption_topics)\n",
        "print('Accuracy score :', acc_score)\n",
        "\n",
        "print('Confusion matrix :')\n",
        "print(sklearn.metrics.confusion_matrix(test_df['Dominant_Topic'].values, pred_caption_topics))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnK3M9HXq9gn"
      },
      "source": [
        "sameTopic = 0\n",
        "diffTopic = 0\n",
        "sameTopicFake = 0\n",
        "sameTopicReal = 0\n",
        "diffTopicFake = 0\n",
        "diffTopicReal = 0\n",
        "\n",
        "textTopics = test_df['Dominant_Topic'].to_list()\n",
        "imageTopics = results_df['pred_topics'].to_list()\n",
        "labels = test_df['label'].to_list()\n",
        "\n",
        "\n",
        "for i in range(len(textTopics)):\n",
        "  if textTopics[i] == imageTopics[i]:\n",
        "    sameTopic += 1\n",
        "    if labels[i] == 'fake':\n",
        "      sameTopicFake += 1\n",
        "    else:\n",
        "      sameTopicReal += 1\n",
        "  else:\n",
        "    diffTopic += 1\n",
        "    if labels[i] == 'fake':\n",
        "      diffTopicFake += 1\n",
        "    else:\n",
        "      diffTopicReal += 1\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}