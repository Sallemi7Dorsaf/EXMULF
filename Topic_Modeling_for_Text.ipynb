{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdJKSkSyyPXj"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wLhqLSXztJP"
      },
      "source": [
        "pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCviigX3yMfv"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import sklearn\n",
        "import sys\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import ldamodel\n",
        "import gensim.corpora\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.preprocessing import normalize\n",
        "import pickle\n",
        "from gensim.models import LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "import pyLDAvis.gensim_models\n",
        "import warnings\n",
        "from itertools import chain\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lmtzr = WordNetLemmatizer()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from tqdm._tqdm_notebook import tqdm_notebook,tnrange,tqdm\n",
        "from collections import Counter,OrderedDict\n",
        "from gensim import models,corpora\n",
        "from gensim.summarization import summarize,keywords\n",
        "import warnings\n",
        "import pyLDAvis.gensim_models\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pyLDAvis.gensim_models\n",
        "import gensim.models.phrases as gen\n",
        "from gensim.models.coherencemodel import CoherenceModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbDvuL7UyMfy"
      },
      "source": [
        "data=pd.read_csv('tweetsttest.csv')\n",
        "len(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCSJsGrwyMf0"
      },
      "source": [
        "data['length_text'] = data['tweetText'].str.len()\n",
        "sns.distplot(data['length_text'], color=\"r\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8bxbDVSyMf0"
      },
      "source": [
        "docs = [x for x in data['tweetText']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhFK1ta8yMf1"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kcZhxFnyMf1"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lbWQjIHyMf2"
      },
      "source": [
        "# A function to prepare the text for topic modelling\n",
        "def words(text):\n",
        "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
        "    text = regex.sub(\" \", text.lower())\n",
        "    words = text.split(\" \")\n",
        "    words = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in words]\n",
        "    words = [re.sub('\\s+', ' ', sent) for sent in words]\n",
        "    words = [re.sub(\"\\'\", \"\", sent) for sent in words]\n",
        "    words = [w for w in words if not len(w) < 2]\n",
        "    words = [w for w in words if w not in stop_words]\n",
        "    words = [lmtzr.lemmatize(w) for w in words]\n",
        "\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAUCVmUhyMf2"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "docs = [words(x) for x in data['tweetText']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW3xxQw0yMf3"
      },
      "source": [
        " # Create a dictionary representation of the documents.\n",
        "dictionary = Dictionary(docs)\n",
        "print('Number of unique words in initital documents:', len(dictionary))\n",
        "\n",
        "# Filter out words that occur less than 10 documents, or more than 20% of the documents.\n",
        "dictionary.filter_extremes(no_below=10, no_above=0.2)\n",
        "print('Number of unique words after removing rare and common words:', len(dictionary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCgK4xRnyMf3"
      },
      "source": [
        "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
        "#print(len(corpus))\n",
        "#corpus[336]\n",
        "bow_doc_300 = corpus[300]\n",
        "\n",
        "for i in range(len(bow_doc_300)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_300[i][0],\n",
        "                                                     dictionary[bow_doc_300[i][0]],\n",
        "                                                     bow_doc_300[i][1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5QxauSayMf4"
      },
      "source": [
        "def get_lda_topics(model, num_topics):\n",
        "    word_dict = {};\n",
        "    for i in range(num_topics):\n",
        "        words = model.show_topic(i, topn = 20);\n",
        "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words];\n",
        "    return pd.DataFrame(word_dict);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBZMoyfpyMf5"
      },
      "source": [
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "\n",
        "                                           num_topics=10,\n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=500,\n",
        "                                           passes=20,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIa2OlgQyMf4"
      },
      "source": [
        "get_lda_topics(lda_model, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpkUQ63hyMf5"
      },
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wzq39maByMf5"
      },
      "source": [
        "pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8ET-DHCyMf5"
      },
      "source": [
        "lda_model.save('model10.gensim')\n",
        "topics = lda_model.print_topics(num_words=6)\n",
        "for topic in topics:\n",
        "    print(topic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UENK6GCGyMf6"
      },
      "source": [
        "#Performance evaluation by classifying sample document using LDA Bag of Words model\n",
        "for index, score in sorted(lda_model[corpus[300]], key=lambda tup: -1*tup[1]) :\n",
        "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 40)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}